{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oleg/yodas_conversion\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "%cd ..\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Literal, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk, Dataset, concatenate_datasets\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.special import log_softmax, expit\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.spectrogram_transformer import AudioSpectrogramTransformer, SEGMENT_SHIFT, SEGMENT_LENGTH\n",
    "from src.yodas_search import YodasSearch, YodasSearchResult\n",
    "from src.audioset_utils import AudioSetOnthology, sliding_window_mean, Features\n",
    "from src.speaker_diarization import Segment, get_speech_mask, reorder_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1169d7fd4db410ab47f4da8f09bcf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d11a535ff343ca9bcd8535f54c3043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yodas2 = concatenate_datasets([\n",
    "    (\n",
    "        concatenate_datasets([load_from_disk(path) for path in tqdm(sorted(Path(dataset_path).glob('*-of-*')))])\n",
    "        .map(lambda s: {'audio': {'path': dataset_path + '/' + s['audio']['path']}})\n",
    "    ) #.take(100)\n",
    "    for dataset_path in [\n",
    "        'datasets/yodas2_ru000_32k',\n",
    "        'datasets/yodas2_ru001_32k'\n",
    "    ]\n",
    "])\n",
    "\n",
    "# yodas2 = load_from_disk('datasets/yodas2_ru000_32k/00000-of-00500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = YodasSearch(\n",
    "    dataset=yodas2,\n",
    "    onthology=AudioSetOnthology(),\n",
    "    cache_file='tmp/search_features.pkl',\n",
    "    # rewrite_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "  text\n",
      "    ndarray(shape=(52553,), dtype=object, obj_type=<class 'str'>)\n",
      "  video_id\n",
      "    ndarray(shape=(52553,), dtype=<U11)\n",
      "  duration\n",
      "    ndarray(shape=(52553,), dtype=float32)\n",
      "  speaker_embeddings\n",
      "    ndarray(shape=(52553,), dtype=object, obj_type=<class 'numpy.ndarray'>)\n",
      "  is_music\n",
      "    Features(52553 samples, shape (1194474, 155), bool, 177 MB, start 0 sec, delta 20 sec, length 120 sec)\n",
      "  acoustic\n",
      "    Features(52553 samples, shape (1194474, 366), bool, 417 MB, start 0 sec, delta 20 sec, length 120 sec)\n",
      "  warnings\n",
      "    Features(52553 samples, shape (1194474, 21), bool, 24 MB, start 0 sec, delta 20 sec, length 120 sec)\n",
      "  for_manual_search\n",
      "    Features(52553 samples, shape (1194474, 3), float32, 14 MB, start 0 sec, delta 20 sec, length 120 sec)\n",
      "  speech_seconds\n",
      "    Features(52553 samples, shape (1194474, 1), float32, 5 MB, start 0 sec, delta 20 sec, length 120 sec)\n",
      "  voice_overlap_seconds\n",
      "    Features(52553 samples, shape (1194474, 1), float32, 5 MB, start 0 sec, delta 20 sec, length 120 sec)\n",
      "  top_speaker_ratio\n",
      "    Features(52553 samples, shape (1194474, 1), float32, 5 MB, start 0 sec, delta 20 sec, length 120 sec)\n",
      "  top_speaker_idx\n",
      "    Features(52553 samples, shape (1194474, 1), int64, 9 MB, start 0 sec, delta 20 sec, length 120 sec)\n"
     ]
    }
   ],
   "source": [
    "searcher.print_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Features(52553 samples, shape (1194474, 1), bool, 1 MB, start 0 sec, delta 20 sec, length 120 sec)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher._features['is_music'].reduce_by_feature_axis('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
